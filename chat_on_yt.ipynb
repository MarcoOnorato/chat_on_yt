{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This application is based on the Ollama service.\n",
    "\n",
    "Just follow the download guide on https://ollama.com/ and then pull the model you need using the shell command \"ollama pull <MODEL NAME>\"\n",
    "\n",
    "This application uses the model llama3.2:3b by default, it needs 4gb of GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain.chains.base import Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS TO GET THE SPOKEN CONTENT OF THE VIDEO AND SUMMARIZE IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_lang_code(lang_codes: list[str], preferences: list[str] = [\"en\"]) -> str:\n",
    "    if lang_codes:\n",
    "        for pref in preferences:\n",
    "            if pref in lang_codes:\n",
    "                return pref\n",
    "\n",
    "        return lang_codes[0]\n",
    "    else:\n",
    "        lang_code = \"\"\n",
    "\n",
    "    return lang_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_texts(video_id: str, languages: list[str]) -> list[str]:\n",
    "    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages = languages)\n",
    "    full_text = '.\\n'.join([sd.get(\"text\",\"\") for sd in transcript])\n",
    "\n",
    "    text_splitter = TokenTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "    texts = text_splitter.split_text(full_text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_from_video(url: str) -> list[str]:\n",
    "    video_id = url.split(\"?v=\")[-1]\n",
    "    lang_codes = [t.language_code for t in YouTubeTranscriptApi.list_transcripts(video_id)]\n",
    "    lang = get_video_lang_code(lang_codes)\n",
    "    texts = get_split_texts(video_id, [lang])\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summarization(url: str, texts: list[str], summarization_chain: Chain) -> str:\n",
    "    summarizations = []\n",
    "    for text in texts:\n",
    "        response = summarization_chain.invoke(\n",
    "            {\n",
    "                \"input_text\":text,\n",
    "                \"n_tokens\": 1000\n",
    "            }\n",
    "        )\n",
    "        summarizations.append(response)\n",
    "\n",
    "    return \"\\n\\n\".join(summarizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an experienced assistant in the synthesis of long texts. Return your answer without adding other informations or reasoning\"),\n",
    "        (\"user\", \"Summarize briefly this text: {input_text} in {n_tokens} words\")\n",
    "    ]\n",
    ")\n",
    "model_name = \"llama3.2\"\n",
    "llm = OllamaLLM(model=model_name, temperature = 0.2)\n",
    "embeddings = OllamaEmbeddings(model=model_name)\n",
    "summarization_chain = summarization_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET SUMMARIZATION OF YOUTUBE VIDEO SPEECH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert YouTube URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.youtube.com/watch?v=i_LwzRVP7bg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = get_texts_from_video(url)\n",
    "summary = get_summarization(url, texts, summarization_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAT ON VIDEO SPEECH CONTENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG functions and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "_ = vector_store.add_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "Return your answer without adding other informations or reasoning.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context} \n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(rag_prompt)\n",
    "\n",
    "rag_chain = rag_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_on_rag(question: str, rag_chain: Chain = rag_chain, k: int = 7) -> str:\n",
    "    context = vector_store.similarity_search(question, k=k)\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in context)\n",
    "    response = rag_chain.invoke(\n",
    "        {\n",
    "            \"question\": question,\n",
    "            \"context\": docs_content\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What techniques of machine learning are explained?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chat_on_rag(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
